{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Chat System - Backend Deployment\n",
        "\n",
        "This notebook deploys the FastAPI backend for the PDF Chat System.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Enable GPU: Runtime → Change runtime type → GPU (T4 or better)\n",
        "2. Set your Hugging Face token in the environment variables cell\n",
        "3. Run all cells sequentially\n",
        "4. Use the public URL provided at the end for your frontend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install fastapi uvicorn python-multipart pydantic PyMuPDF sentence-transformers transformers torch numpy faiss-cpu rank-bm25 tiktoken bitsandbytes accelerate scipy scikit-learn pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "\n",
        "# IMPORTANT: Replace with your actual Hugging Face token\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_huggingface_token_here\"\n",
        "\n",
        "print(\"Environment variables set successfully!\")\n",
        "print(f\"Hugging Face token: {'*' * 20}{os.environ['HUGGINGFACE_HUB_TOKEN'][-4:]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. The system will be very slow.\")\n",
        "    print(\"Please enable GPU: Runtime → Change runtime type → GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Instructions\n",
        "\n",
        "### Frontend Setup:\n",
        "1. Clone or download the frontend code\n",
        "2. Set environment variable: `NEXT_PUBLIC_API_URL={public_url}`\n",
        "3. Run: `npm install && npm run dev`\n",
        "\n",
        "### API Endpoints:\n",
        "- `GET /health` - Health check\n",
        "- `POST /upload-pdfs` - Upload PDF files\n",
        "- `POST /ask-question` - Ask questions about PDFs\n",
        "- `GET /status` - Get system status\n",
        "\n",
        "### Important Notes:\n",
        "- Keep this Colab session running while using the application\n",
        "- The ngrok URL will change if you restart the runtime\n",
        "- Free ngrok has limitations on concurrent connections\n",
        "- Models will be downloaded on first use (may take time)\n",
        "\n",
        "### Troubleshooting:\n",
        "- If models fail to load, check your Hugging Face token\n",
        "- If GPU is not available, the system will be very slow\n",
        "- If API is not responding, wait a few minutes for models to load\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
